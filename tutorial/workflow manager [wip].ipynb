{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as dda\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "sys.path.append('../')\n",
    "from sed.core.workflow import WorkflowManager, WorkflowStep\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate a test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randn(1000,3)\n",
    "arr[:,0] = np.random.randint(600,900,1000)\n",
    "df = pd.DataFrame(data=arr,columns=['tof','x','y'])\n",
    "df.head()\n",
    "dsk = ddf.from_pandas(df,npartitions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define a workflow step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tof_to_energy(WorkflowStep):\n",
    "\n",
    "    # init contains the parameters one wants to use\n",
    "    def __init__(\n",
    "        self,\n",
    "        tof_column:str,\n",
    "        tof_offset:float,\n",
    "        tof_distance:float,\n",
    "        energy_offset:float=0,\n",
    "        out_cols='energy',\n",
    "        duplicate_policy='raise'\n",
    "    ) -> None:\n",
    "        self.tof_column = tof_column\n",
    "        self.tof_offset = tof_offset\n",
    "        self.tof_distance = tof_distance\n",
    "        self.energy_offset = energy_offset\n",
    "        super().__init__(\n",
    "            out_cols=out_cols,\n",
    "            duplicate_policy=duplicate_policy,\n",
    "        )\n",
    "    \n",
    "    # can define arbitrary functions which can be called internally\n",
    "\n",
    "    # the main required function. this is called and mapped on the dataframe\n",
    "    # this is the only hard requirement for these classes. all functionality is \n",
    "    # then inherited from the WorkflowStep parent class\n",
    "    def func(self,df: ddf.DataFrame) ->  ddf.DataFrame:\n",
    "        k = 0.5 * 1e18 * 9.10938e-31 / 1.602177e-19\n",
    "        return k * np.power(\n",
    "            self.tof_distance / ((df[self.tof_column]) - self.tof_offset), 2.\n",
    "            ) - self.energy_offset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize the workflow step\n",
    "this is not necessary, but shows it works internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = Tof_to_energy(\n",
    "    tof_column='tof',\n",
    "    tof_offset = 1,\n",
    "    tof_distance = 1,\n",
    "    energy_offset = 100,\n",
    "    out_cols = 'energy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn(dsk).compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now the actua workflow manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sed.core.metadata import MetaHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = WorkflowManager(\n",
    "    dataframe = dsk,\n",
    "    workflow=[fn],# this can be a list of workflow steps which will be performed in order.\n",
    "    metadata=MetaHandler(),\n",
    "    config={'binning':{'num_cores':8}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the workflow\n",
    "this runs through the workflow queue and applies all transformations. If a step was used already, it rises DuplicateEntryError, unless a different duplicate_policy is defined in the workflow_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.apply_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do some binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = wf.compute_binning(\n",
    "    bins=10,\n",
    "    axes=['energy','x','y'],\n",
    "    ranges=[(-96,-93),(-2,2),(-2,2)],\n",
    "    num_cores=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "- define how to handle complex steps which require generating parameters\n",
    "- fetch parameters from config file\n",
    "- create the workflow steps based on the transformations we currently are familiar with (e.g. jittering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sed_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa69adafc7decb85aa669de03cf2f28d516b651fe8c880c327b667ed9adf12b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
